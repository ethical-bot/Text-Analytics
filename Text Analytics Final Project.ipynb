{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMx41DFfxozrl/Ygnc/o+0o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1IEnjIzb7Py","executionInfo":{"status":"ok","timestamp":1727943241317,"user_tz":-330,"elapsed":17401,"user":{"displayName":"Rahul M Singh","userId":"10199658645358329260"}},"outputId":"735d3480-8057-4c64-8b82-52f9820dcc75"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.5175879396984925\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.42      0.33      0.37       171\n","           1       0.57      0.66      0.61       227\n","\n","    accuracy                           0.52       398\n","   macro avg       0.49      0.49      0.49       398\n","weighted avg       0.50      0.52      0.51       398\n","\n","The stock is predicted to decrease.\n"]}],"source":["import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","import nltk\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","file_path = 'Combined_News_DJIA.csv'\n","df = pd.read_csv(file_path)\n","\n","# Combine all news headlines for each day into a single string\n","df['Combined_News'] = df.iloc[:, 2:27].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","\n","    tokens = word_tokenize(text)\n","\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Join tokens back into a string\n","    return ' '.join(tokens)\n","\n","# Apply preprocessing to the combined news headlines\n","df['Processed_News'] = df['Combined_News'].apply(preprocess_text)\n","\n","# Features (TF-IDF Vectorization of the processed news) and Target (Label)\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for optimization\n","X = tfidf_vectorizer.fit_transform(df['Processed_News']).toarray()\n","y = df['Label']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Use Logistic Regression for classification\n","model = LogisticRegression(max_iter=1000, random_state=42)\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","classification_rep = classification_report(y_test, y_pred)\n","\n","# Output results\n","print(f'Accuracy: {accuracy}')\n","print('Classification Report:')\n","print(classification_rep)\n","\n","# Test case: Predicting stock movement based on new input text\n","test_headlines = \"\"\"\n","Apple announces new iPhone; Stock surges.\n","Tesla shares rise after strong earnings report.\n","Market reacts positively to government fiscal policy.\n","\"\"\"\n","# Preprocess the test input\n","test_input_processed = preprocess_text(test_headlines)\n","test_input_tfidf = tfidf_vectorizer.transform([test_input_processed])\n","\n","# Predict using the model\n","test_prediction = model.predict(test_input_tfidf)\n","\n","if test_prediction == 1:\n","    print(\"The stock is predicted to increase.\")\n","else:\n","    print(\"The stock is predicted to decrease.\")"]},{"cell_type":"code","source":["# prompt: For the above code use XG boost and try to increase the accuracy and write the complete code again\n","\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from xgboost import XGBClassifier  # Import XGBoost Classifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import nltk\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","file_path = 'Combined_News_DJIA.csv'\n","df = pd.read_csv(file_path)\n","\n","# Combine all news headlines for each day into a single string\n","df['Combined_News'] = df.iloc[:, 2:27].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","\n","    tokens = word_tokenize(text)\n","\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Join tokens back into a string\n","    return ' '.join(tokens)\n","\n","# Apply preprocessing to the combined news headlines\n","df['Processed_News'] = df['Combined_News'].apply(preprocess_text)\n","\n","# Features (TF-IDF Vectorization of the processed news) and Target (Label)\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for optimization\n","X = tfidf_vectorizer.fit_transform(df['Processed_News']).toarray()\n","y = df['Label']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Use XGBoost Classifier for classification\n","model = XGBClassifier(random_state=42)  # Initialize XGBoost Classifier\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","classification_rep = classification_report(y_test, y_pred)\n","\n","# Output results\n","print(f'Accuracy: {accuracy}')\n","print('Classification Report:')\n","print(classification_rep)\n","\n","# Test case: Predicting stock movement based on new input text\n","test_headlines = \"\"\"\n","Apple announces new iPhone; Stock surges.\n","Tesla shares rise after strong earnings report.\n","Market reacts positively to government fiscal policy.\n","\"\"\"\n","# Preprocess the test input\n","test_input_processed = preprocess_text(test_headlines)\n","test_input_tfidf = tfidf_vectorizer.transform([test_input_processed])\n","\n","# Predict using the model\n","test_prediction = model.predict(test_input_tfidf)\n","\n","if test_prediction == 1:\n","    print(\"The stock is predicted to increase.\")\n","else:\n","    print(\"The stock is predicted to decrease.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxwcJi6Th7pm","executionInfo":{"status":"ok","timestamp":1727944783442,"user_tz":-330,"elapsed":27446,"user":{"displayName":"Rahul M Singh","userId":"10199658645358329260"}},"outputId":"247ba74b-e148-4e74-88bf-b858d9650db9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.507537688442211\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.49      0.46       171\n","           1       0.57      0.52      0.55       227\n","\n","    accuracy                           0.51       398\n","   macro avg       0.50      0.50      0.50       398\n","weighted avg       0.51      0.51      0.51       398\n","\n","The stock is predicted to increase.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import nltk\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","file_path = 'Combined_News_DJIA.csv'\n","df = pd.read_csv(file_path)\n","\n","# Combine all news headlines for each day into a single string\n","df['Combined_News'] = df.iloc[:, 2:27].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n","\n","def preprocess_text(text):\n","\n","    text = text.lower()\n","\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","\n","    tokens = word_tokenize(text)\n","\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Join tokens back into a string\n","    return ' '.join(tokens)\n","\n","# Apply preprocessing to the combined news headlines\n","df['Processed_News'] = df['Combined_News'].apply(preprocess_text)\n","\n","# Features (TF-IDF Vectorization of the processed news) and Target (Label)\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for optimization\n","X = tfidf_vectorizer.fit_transform(df['Processed_News']).toarray()\n","y = df['Label']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Random Forest Classifier\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","rf_y_pred = rf_model.predict(X_test)\n","rf_accuracy = accuracy_score(y_test, rf_y_pred)\n","rf_classification_rep = classification_report(y_test, rf_y_pred)\n","\n","print(\"Random Forest Results:\")\n","print(f'Accuracy: {rf_accuracy}')\n","print('Classification Report:')\n","print(rf_classification_rep)\n","\n","# Gradient Boosting Classifier\n","gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","gb_model.fit(X_train, y_train)\n","gb_y_pred = gb_model.predict(X_test)\n","gb_accuracy = accuracy_score(y_test, gb_y_pred)\n","gb_classification_rep = classification_report(y_test, gb_y_pred)\n","\n","print(\"\\nGradient Boosting Results:\")\n","print(f'Accuracy: {gb_accuracy}')\n","print('Classification Report:')\n","print(gb_classification_rep)\n","\n","# Test case: Predicting stock movement based on new input text (using Random Forest)\n","test_headlines = \"\"\"\n","Apple announces new iPhone; Stock surges.\n","Tesla shares rise after strong earnings report.\n","Market reacts positively to government fiscal policy.\n","\"\"\"\n","# Preprocess the test input\n","test_input_processed = preprocess_text(test_headlines)\n","test_input_tfidf = tfidf_vectorizer.transform([test_input_processed])\n","\n","# Predict using the Random Forest model\n","test_prediction = rf_model.predict(test_input_tfidf)\n","\n","if test_prediction == 1:\n","    print(\"\\nThe stock is predicted to increase (Random Forest).\")\n","else:\n","    print(\"\\nThe stock is predicted to decrease (Random Forest).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqDxkiU0ckru","executionInfo":{"status":"ok","timestamp":1727943443369,"user_tz":-330,"elapsed":51350,"user":{"displayName":"Rahul M Singh","userId":"10199658645358329260"}},"outputId":"8d994241-60b7-4019-8fdc-d66bcb5c4530"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Random Forest Results:\n","Accuracy: 0.49748743718592964\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.36      0.38       171\n","           1       0.56      0.60      0.58       227\n","\n","    accuracy                           0.50       398\n","   macro avg       0.48      0.48      0.48       398\n","weighted avg       0.49      0.50      0.49       398\n","\n","\n","Gradient Boosting Results:\n","Accuracy: 0.507537688442211\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.42      0.38      0.40       171\n","           1       0.56      0.60      0.58       227\n","\n","    accuracy                           0.51       398\n","   macro avg       0.49      0.49      0.49       398\n","weighted avg       0.50      0.51      0.50       398\n","\n","\n","The stock is predicted to decrease (Random Forest).\n"]}]}]}